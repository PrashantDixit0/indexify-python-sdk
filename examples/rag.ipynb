{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from indexify import IndexifyClient\n",
    "from openai import OpenAI\n",
    "import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_summaries = load_dataset(\"d0rj/wikisum\")\n",
    "data = wiki_summaries['train']\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexify_client = IndexifyClient()\n",
    "for summary in data['summary']:\n",
    "    indexify_client.add_documents(summary)\n",
    "\n",
    "indexify_client.add_extraction_policy(\n",
    "    extractor=\"tensorlake/minilm-l6\", name=\"minilml6\", content_source=\"ingestion\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(indexify_client.get_content()))\n",
    "# docs = indexify_client.search_index(\n",
    "#     \"minilml6.embedding\",\n",
    "#     \"How to store oysters\",\n",
    "#     2)\n",
    "\n",
    "# print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotpot = load_dataset(\"hotpot_qa\", \"fullwiki\")\n",
    "hotpot_subset = hotpot['validation'][:100]\n",
    "questions = hotpot_subset['question']\n",
    "correct_answers = hotpot_subset['answer']\n",
    "questions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "class HotPotQARAG:\n",
    "\n",
    "    def __init__(self, indexify_client, openai_client):\n",
    "        self.openai_client = openai_client\n",
    "        self.indexify_client = indexify_client\n",
    "        self.num_docs = 3\n",
    "\n",
    "    def query_index(self, question):\n",
    "\n",
    "        return self.indexify_client.search_index(\n",
    "            \"minilml6.embedding\",\n",
    "            question,\n",
    "            self.num_docs\n",
    "        )\n",
    "\n",
    "    def get_answers(self, questions):\n",
    "        answers = []\n",
    "        for question in tqdm.tqdm(questions):\n",
    "            response = self.openai_client.chat.completions.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"Answer the following question: Don't be verbose, or make full sentences. Just answer the question .\"},\n",
    "                    {\"role\": \"user\", \"content\": question}\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            answers.append(response.choices[0].message.content)\n",
    "\n",
    "        return answers\n",
    "    \n",
    "    def get_answers_with_context(self, questions):\n",
    "        answers = []\n",
    "        for question in tqdm.tqdm(questions):\n",
    "            docs = self.query_index(question)\n",
    "            context = \" \".join([doc['text'] for doc in docs])\n",
    "            response = self.openai_client.chat.completions.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"Answer the following question: Don't be verbose, or make full sentences. Just answer the question. You can use the context provided if you find it helpful.\"},\n",
    "                    {\"role\": \"user\", \"content\": f\"{question} #### Context {context}\"}\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            answers.append(response.choices[0].message.content)\n",
    "\n",
    "        return answers\n",
    "    \n",
    "    def check_match(self, answers, correct_answers):\n",
    "        match = []\n",
    "        for answer, correct in zip(answers, correct_answers):\n",
    "            if answer in correct or correct in answer:\n",
    "                match.append(True)\n",
    "            else:\n",
    "                match.append(False)\n",
    "\n",
    "        return match\n",
    "\n",
    "\n",
    "    def check_llm_match(self, answers, correct_answers):\n",
    "\n",
    "        match = []\n",
    "        for answer, correct_answer in tqdm.tqdm(list(zip(answers, correct_answers))):\n",
    "            response = self.openai_client.chat.completions.create(\n",
    "                        model=\"gpt-3.5-turbo\",\n",
    "                        messages=[\n",
    "                            {\"role\": \"system\", \"content\": \"Check if the answer means the same as the correct answer. \\\n",
    "                            They need not be exact matches. Return True if they mean the same, and False if they are different.\"},\n",
    "                            {\"role\": \"user\", \"content\": f'answer: {answer} correct answer: {correct_answer}'}\n",
    "                        ]\n",
    "                    )\n",
    "            \n",
    "            match.append(response.choices[0].message.content == \"True\")\n",
    "            time.sleep(0.5)\n",
    "\n",
    "        return match\n",
    "\n",
    "    @staticmethod\n",
    "    def _process_answers(answers):\n",
    "        return [answer.lower().replace('.', '') for answer in answers]\n",
    "\n",
    "    \n",
    "    def evaluate_answers(self, answers, correct_answers):\n",
    "        exact_matches = self.check_match(answers, correct_answers)\n",
    "        llm_matches = self.check_llm_match(answers, correct_answers)\n",
    "\n",
    "        em = sum(exact_matches) / len(exact_matches)\n",
    "        llm_match = sum(llm_matches) / len(llm_matches)\n",
    "\n",
    "        return em, llm_match\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_key = \"OPENAI_API_KEY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_client = OpenAI(api_key=openai_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_system = HotPotQARAG(indexify_client, openai_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = rag_system.get_answers(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers_with_context = rag_system.get_answers_with_context(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = rag_system.evaluate_answers(answers, correct_answers)\n",
    "print(f'Exact Match Evaluation: {results[0]}, LLM Match Evaluation: {results[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_context = rag_system.evaluate_answers(answers_with_context, correct_answers)\n",
    "print(f'Exact Match Evaluation: {results_context[0]}, LLM Match Evaluation: {results_context[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ve",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
